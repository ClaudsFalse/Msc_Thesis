{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Claudia Falsetti <1431314>\n",
    "# Python 3.7.3 \n",
    "\n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords, words, wordnet \n",
    "from gensim import corpora, models, similarities\n",
    "from string import punctuation, digits\n",
    "from collections import Counter, defaultdict\n",
    "import re \n",
    "import numpy as np \n",
    "import csv \n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis.gensim\n",
    "import numpy as np \n",
    "from time import time \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(filename):\n",
    "\n",
    "    data_frame = pd.read_csv(filename, names=[\"TweetID\",\"Username\", \"Text\"])\n",
    "    df = data_frame.sort_values(by='Username').drop_duplicates('Text')\n",
    "    data_frame_new = df.iloc[:,1:3]\n",
    "    #print(data_frame_new.head(5))\n",
    "\n",
    "    usernames = data_frame_new.iloc[:,0]\n",
    "\n",
    "    usernames_list = set(usernames.values.tolist())\n",
    "\n",
    "\n",
    "    documents = data_frame_new.groupby('Username')['Text'].apply(list).to_dict()\n",
    "\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(documents):\n",
    "    \"\"\"\n",
    "    Function that lowercase the text and clean it\n",
    "    Break the sentences into tokens\n",
    "    remove punctuations and stopwords\n",
    "    Standardise the text : can't -> cannot, I'm -> I am\n",
    "    \"\"\"\n",
    "\n",
    "    #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    token_frequency = defaultdict(int)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = set(wordnet.all_lemma_names())\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    stoplist_extra=['amp','youd', 'wed' ,'id', 'get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
    "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via', 'seven', 'eight', 'nine', 'ten',\n",
    "            'one','com','new','like','great','make','top','good','wow','yes','say','yay','would','thanks','thank','going',\n",
    "            'new','use','should','could','best','really','see','want','nice', ' shes', 'hes ', 'were', 'theyre', 'yous',\n",
    "            'two', 'three','four', 'five', 'six', 'while','know', 'ngl', 'brb', 'acc', 'smh', 'fwiw', 'ftl', \n",
    "            'lmao', 'lol', 'omg', 'https', 'mvp', 'isnt', 'arent', 'ever','cant', 'hnd', 'sbe', 'gsa', 'bwr', 'yeah', 'fuck', 'torture',\n",
    "            'shit','say', 'try', 'well', 'take', 'way', 'many', 'yet','never', 'may', 'come', 'actually', 'much', 'thing', 'year', 'month',\n",
    "            'people', 'also', 'around', 'think', 'keep', 'time', 'someone', 'give', 'back', 'need', 'time', 'feel', 'look', 'even', \n",
    "            'start', 'every', 'tell', 'first', 'lot', 'sure', 'though', 'end', 'still', 'bitch',\n",
    "                   'wait', 'watch', 'already', 'always', 'thought','right', 'call', 'put', 'long',\n",
    "                   'mak', 'cunt', 'aku', 'dia', 'yang','dah', 'masa', 'lepas', 'dari', \n",
    "                   'joseph', 'miller', 'ada', 'orang', 'kat', 'lah','zaman','gila','akan','tau','salah','mak','kali','ali',\n",
    "                   'susah','lama','bahasa', 'het']\n",
    "\n",
    "        # identify bigrams and unigrams to strip from tweets \n",
    "\n",
    "    counter = 0 \n",
    "\n",
    "    with open('new_one.csv', 'a') as csvFile:\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        for username in documents.keys():\n",
    "\n",
    "            #print(\"this is a key\", username)\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "           # print('Number of iterations: ' + str(counter) + \"  out of:  \" + str(len(usernames_list)))\n",
    "\n",
    "            documents[username] = \" \".join(documents[username]).lower().split()\n",
    "            #documents[username] = [re.sub(r\"em\", \"email \", str(documents[username]))]\n",
    "        \n",
    "            #documents[username] = [re.sub(r\"til\", \"learned \", str(documents[username]))]\n",
    "            documents[username] = [re.sub(r\"fb\", \"facebook \", word) for word in documents[username]]   # look for twitter most used acronyms\t\t\t\t\t\t\n",
    "            documents[username] = [re.sub(r\"jk\", \"joking \", word) for word in documents[username]]\n",
    "            documents[username] = [re.sub(r'@[^\\s]+','', word) for word in documents[username]]\n",
    "            #documents[username] = [re.sub(r'#[^\\s]+','', word) for word in documents[username]]\n",
    "\n",
    "\n",
    "            # remove punctuation and replace with space and remove links \n",
    "            documents[username] = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE) for word in documents[username]]\n",
    "            documents[username] = [re.sub(r\"[\\\"'-,.;@?!&$:'/]+\\ *\", \" \", word) for word in documents[username]]\n",
    "\n",
    "\n",
    "            # remove digits \n",
    "\n",
    "            documents[username] = [re.sub(r'\\w*\\d\\w*', '', word) for word in documents[username]]\n",
    "\n",
    "            print(\"creating unigrams and stopwords list.....\")\n",
    "\n",
    "            # remove stopwords \n",
    "            documents[username] =  ' '.join(documents[username]).split()\n",
    "\n",
    "            unigrams = [ word for word in documents[username] if len(word)==1]\n",
    "            bigrams  = [ word for word in documents[username] if len(word)==2]\n",
    "            stoplist  = set(stop_words + stoplist_extra + unigrams + bigrams)\n",
    "\n",
    "            documents[username] = [word for word in documents[username] if word not in stoplist]\n",
    "\n",
    "\n",
    "            #print(\"Stopwords have been removed \")\n",
    "            \n",
    "            documents[username] = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in documents[username]]\n",
    "            documents[username] = [word for word in documents[username] if word in lemmas]\n",
    "            documents[username] = [word for word in documents[username] if word not in stoplist]\n",
    "\n",
    "\n",
    "            docs = [documents[username] for username in documents.keys()]\n",
    "\n",
    "            csvWriter.writerow([username, documents[username]])\n",
    "    csvFile.close()\n",
    "\n",
    "\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    df['text'] = strip_newline(df.text)\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
